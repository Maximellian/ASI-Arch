#!/bin/bash

# train.sh: Robust MPS-compatible training runner with CSV accumulation

DEBUG_LOG="pipeline/files/debug/training_error.txt"
OUTPUT_LOG="pipeline/files/debug/training_output.txt"
EXPERIMENT_NAME="$1"

# Ensure all output directories exist
mkdir -p "$(dirname "$DEBUG_LOG")"
mkdir -p "pipeline/files/analysis"

# Initialize/clear per-run log files (overwrites on each run)
: > "$DEBUG_LOG"
: > "$OUTPUT_LOG"

echo "[$(date)] Starting MPS-compatible training runner for: $EXPERIMENT_NAME" | tee -a "$DEBUG_LOG" | tee -a "$OUTPUT_LOG"

if [ -z "$EXPERIMENT_NAME" ]; then
  echo "Error: No experiment name provided" | tee -a "$DEBUG_LOG"
  echo "Usage: $0 <experiment_name>" | tee -a "$DEBUG_LOG"
  exit 1
fi

# --- Environment Snapshot ---
echo "=== ENVIRONMENT SNAPSHOT ===" | tee -a "$OUTPUT_LOG"
python3 - << 'PYCODE' | tee -a "$OUTPUT_LOG"
import sys, pkg_resources
print("Python executable:", sys.executable)
for pkg in pkg_resources.working_set:
    print(f"{pkg.key}: {pkg.version}")
PYCODE
echo "" | tee -a "$OUTPUT_LOG"

# --- Write the temporary Python runner ---
cat << 'EOF' > /tmp/mps_training_runner_accumulate.py
#!/usr/bin/env python3

import sys, os, importlib.util, traceback, csv, logging
from datetime import datetime
from typing import List, Dict, Any

import torch

# Logging setup
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

sys.path.insert(0, os.path.abspath('.'))

if torch.backends.mps.is_available():
    logger.info("MPS device detected - disabling torch.compile for compatibility")
    try:
        torch._dynamo.config.suppress_errors = True
        torch._dynamo.config.disable = True
    except Exception as e:
        logger.warning(f"Could not disable torch.compile: {e}")

def setup_output_files():
    loss_file = 'pipeline/files/analysis/loss.csv'
    benchmark_file = 'pipeline/files/analysis/benchmark.csv'
    loss_headers = [
        'epoch', 'batch', 'train_loss', 'val_loss', 'learning_rate', 'timestamp', 'experiment_name'
    ]
    benchmark_headers = [
        'experiment_name','model_params','forward_time_ms','memory_mb','throughput_samples_sec',
        'accuracy','perplexity','timestamp','input_shape','output_shape'
    ]
    if not os.path.exists(loss_file) or os.path.getsize(loss_file) == 0:
        with open(loss_file, 'w', newline='') as f:
            csv.writer(f).writerow(loss_headers)
        logger.info("Initialized new loss.csv with headers")
    if not os.path.exists(benchmark_file) or os.path.getsize(benchmark_file) == 0:
        with open(benchmark_file, 'w', newline='') as f:
            csv.writer(f).writerow(benchmark_headers)
        logger.info("Initialized new benchmark.csv with headers")

def save_metrics(training_metrics: List[Dict], benchmark_results: List[Dict]):
    if training_metrics:
        with open('pipeline/files/analysis/loss.csv', 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'epoch','batch','train_loss','val_loss','learning_rate','timestamp','experiment_name'])
            for m in training_metrics:
                writer.writerow(m)
        logger.info(f"Appended {len(training_metrics)} training metrics to loss.csv")
    if benchmark_results:
        with open('pipeline/files/analysis/benchmark.csv', 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'experiment_name','model_params','forward_time_ms','memory_mb','throughput_samples_sec',
                'accuracy','perplexity','timestamp','input_shape','output_shape'])
            for r in benchmark_results:
                writer.writerow(r)
        logger.info(f"Appended {len(benchmark_results)} benchmark results to benchmark.csv")

def main():
    try:
        setup_output_files()
        if len(sys.argv) < 2:
            logger.error("No experiment name provided")
            sys.exit(1)
        experiment_name = sys.argv[1]
        logger.info(f"Running experiment: {experiment_name}")
        # TODO: Add your actual training/experiment logic here
        # Sample metrics for demonstration
        training_metrics = [{
            'epoch': 1,
            'batch': 100,
            'train_loss': 0.5,
            'val_loss': 0.6,
            'learning_rate': 0.001,
            'timestamp': datetime.now().isoformat(),
            'experiment_name': experiment_name
        }]
        benchmark_results = [{
            'experiment_name': experiment_name,
            'model_params': 1000000,
            'forward_time_ms': 10.5,
            'memory_mb': 128,
            'throughput_samples_sec': 100.0,
            'accuracy': 0.95,
            'perplexity': 25.0,
            'timestamp': datetime.now().isoformat(),
            'input_shape': '[1, 128, 512]',
            'output_shape': '[1, 128, 512]'
        }]
        save_metrics(training_metrics, benchmark_results)
        print("âœ” Training completed with metrics accumulated in CSV files")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Training failed with error: {e}")
        traceback.print_exc()
        sys.exit(2)

if __name__ == "__main__":
    main()
EOF

chmod +x /tmp/mps_training_runner_accumulate.py

# --- Actually run the experiment ---
python3 /tmp/mps_training_runner_accumulate.py "$EXPERIMENT_NAME" >>"$OUTPUT_LOG" 2>>"$DEBUG_LOG"
EXIT_CODE=$?

if [[ $EXIT_CODE -eq 0 ]]; then
  echo "[$(date)] Experiment completed successfully." | tee -a "$DEBUG_LOG" | tee -a "$OUTPUT_LOG"
else
  echo "[$(date)] Experiment failed with exit code $EXIT_CODE (see $DEBUG_LOG for details)." | tee -a "$DEBUG_LOG" | tee -a "$OUTPUT_LOG"
fi

# --- Clean up temporary runner ---
rm -f /tmp/mps_training_runner_accumulate.py

exit $EXIT_CODE
